name: Deploy to Databricks

# Trigger on push to main branch
on:
  push:
    branches:
      - main
  workflow_dispatch:  # Allow manual trigger from GitHub UI

# Environment variables (from GitHub Secrets)
env:
  DATABRICKS_HOST: ${{ secrets.DATABRICKS_HOST }}
  DATABRICKS_TOKEN: ${{ secrets.DATABRICKS_TOKEN }}

jobs:
  # ===========================
  # JOB 1: SYNC CODE TO DATABRICKS
  # ===========================
  sync-to-databricks:
    name: Sync Notebooks to Databricks Workspace
    runs-on: ubuntu-latest

    steps:
      # Step 1: Checkout code from GitHub
      - name: Checkout code
        uses: actions/checkout@v4

      # Step 2: Setup Python
      - name: Setup Python 3.11
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'

      # Step 3: Install Databricks CLI
      - name: Install Databricks CLI
        run: |
          pip install databricks-cli

      # Step 4: Configure Databricks CLI
      - name: Configure Databricks CLI
        run: |
          echo "[DEFAULT]" > ~/.databrickscfg
          echo "host = $DATABRICKS_HOST" >> ~/.databrickscfg
          echo "token = $DATABRICKS_TOKEN" >> ~/.databrickscfg

      # Step 5: Create or update Databricks Repo
      - name: Sync code to Databricks Repos
        run: |
          # Try to update existing repo, create if doesn't exist
          REPO_PATH="/Repos/Production/azure-rag-heineken"

          # Check if repo exists
          if databricks repos get --path "$REPO_PATH" 2>/dev/null; then
            echo "Repo exists, updating..."
            databricks repos update --path "$REPO_PATH" --branch main
          else
            echo "Repo doesn't exist, creating..."
            databricks repos create \
              --url https://github.com/Matin0SH/azure-rag-heineken \
              --provider gitHub \
              --path "$REPO_PATH"
          fi

          echo "✅ Code synced to Databricks workspace at $REPO_PATH"

  # ===========================
  # JOB 2: DEPLOY INFRASTRUCTURE WITH TERRAFORM
  # ===========================
  deploy-terraform:
    name: Deploy Infrastructure with Terraform
    runs-on: ubuntu-latest
    needs: sync-to-databricks  # Run after code sync completes

    # Only run Terraform if terraform files changed
    # (Comment out this 'if' to always run Terraform)
    if: |
      contains(github.event.head_commit.modified, 'terraform/') ||
      contains(github.event.head_commit.added, 'terraform/')

    steps:
      # Step 1: Checkout code
      - name: Checkout code
        uses: actions/checkout@v4

      # Step 2: Setup Terraform
      - name: Setup Terraform
        uses: hashicorp/setup-terraform@v3
        with:
          terraform_version: 1.6.0

      # Step 3: Configure Azure credentials (for Terraform backend)
      - name: Azure Login
        uses: azure/login@v1
        with:
          creds: ${{ secrets.AZURE_CREDENTIALS }}

      # Step 4: Terraform Init (with Azure backend)
      - name: Terraform Init
        working-directory: ./terraform
        run: terraform init

      # Step 5: Terraform Format Check
      - name: Terraform Format Check
        working-directory: ./terraform
        run: terraform fmt -check
        continue-on-error: true  # Don't fail if formatting is off

      # Step 6: Terraform Validate
      - name: Terraform Validate
        working-directory: ./terraform
        run: terraform validate

      # Step 7: Terraform Plan
      - name: Terraform Plan
        working-directory: ./terraform
        env:
          TF_VAR_databricks_host: ${{ secrets.DATABRICKS_HOST }}
          TF_VAR_databricks_token: ${{ secrets.DATABRICKS_TOKEN }}
          TF_VAR_catalog_name: heineken_test_workspace
          TF_VAR_schema_name: nextlevel-rag
          TF_VAR_environment: dev
          TF_VAR_vector_search_endpoint: heineken-vdb
          TF_VAR_vector_index_name: chunks_embedded_index
          TF_VAR_embedding_model: databricks-gte-large-en
          TF_VAR_notebook_path: /Repos/Production/azure-rag-heineken/jobs/ingest_pipeline
          TF_VAR_existing_cluster_id: ""
        run: terraform plan -out=tfplan

      # Step 8: Terraform Apply
      - name: Terraform Apply
        working-directory: ./terraform
        env:
          TF_VAR_databricks_host: ${{ secrets.DATABRICKS_HOST }}
          TF_VAR_databricks_token: ${{ secrets.DATABRICKS_TOKEN }}
          TF_VAR_catalog_name: heineken_test_workspace
          TF_VAR_schema_name: nextlevel-rag
          TF_VAR_environment: dev
          TF_VAR_vector_search_endpoint: heineken-vdb
          TF_VAR_vector_index_name: chunks_embedded_index
          TF_VAR_embedding_model: databricks-gte-large-en
          TF_VAR_notebook_path: /Repos/Production/azure-rag-heineken/jobs/ingest_pipeline
          TF_VAR_existing_cluster_id: ""
        run: terraform apply -auto-approve tfplan

      # Step 9: Output Terraform results
      - name: Terraform Output
        working-directory: ./terraform
        run: terraform output

  # ===========================
  # JOB 3: POST-DEPLOYMENT VALIDATION
  # ===========================
  validate-deployment:
    name: Validate Deployment
    runs-on: ubuntu-latest
    needs: [sync-to-databricks, deploy-terraform]

    steps:
      # Step 1: Checkout code
      - name: Checkout code
        uses: actions/checkout@v4

      # Step 2: Install Databricks CLI
      - name: Install Databricks CLI
        run: pip install databricks-cli

      # Step 3: Configure Databricks CLI
      - name: Configure Databricks CLI
        run: |
          echo "[DEFAULT]" > ~/.databrickscfg
          echo "host = $DATABRICKS_HOST" >> ~/.databrickscfg
          echo "token = $DATABRICKS_TOKEN" >> ~/.databrickscfg

      # Step 4: Verify Databricks Repo exists
      - name: Verify Databricks Repo
        run: |
          REPO_PATH="/Repos/Production/azure-rag-heineken"
          if databricks repos get --path "$REPO_PATH"; then
            echo "✅ Databricks Repo verified at $REPO_PATH"
          else
            echo "❌ Databricks Repo not found"
            exit 1
          fi

      # Step 5: List Databricks jobs (verify Terraform created them)
      - name: Verify Databricks Jobs
        run: |
          echo "Listing Databricks jobs..."
          databricks jobs list --output JSON | jq '.jobs[] | {job_id, name: .settings.name}'
          echo "✅ Databricks jobs verified"
